---
layout: default
section: home
title: "Yang Chen"
---

<style>
  /* ... (your existing styles) */
  #toggleButton {
    background-color: #123c40;
    color: white;
    padding: 10px 15px;
    margin: 5px 0;
    border: none;
    cursor: pointer;
    border-radius: 4px;
    font-size: 14px;
  }
</style>

<style>
  /* For large screens */
  .paper {
      display: flex;
      flex-direction: row;
      justify-content: space-between;
  }
  .paper img {
      width: 25%; /* Adjust as needed */
  }
  .paper div {
      width: 70%; /* Adjust as needed */
  }

  /* For screens smaller than or equal to 700px */
  @media screen and (max-width: 700px) {
      .paper {
          flex-direction: column;
          align-items: left;
      }
      .paper img {
          width: 50%;
          height: auto; /* Maintain aspect ratio */
      }
      .paper div {
          width: 100%;
          text-align: left; /* Optional: center align the text */
      }
  }
</style>

<style>
  li.paper {
    margin-left: 20px;
    margin-bottom: 10px;
  }

  li.paper .title {
    font-size: 115%;
  }

  li.paper .me {
    background-color: #eeffee;
  }
</style>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-0KCXSPDD83"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-0KCXSPDD83');
</script>

<div class="row">
  <div class="col-sm-3">
    <!-- <img src="static/yang.jpeg" class="img-thumbnail"
    style="margin-top: 35px; margin-bottom: 15px"> -->
    <img src="static/yang.png"  width="200"
         height="190" style="margin-top: 25px; margin-bottom: 15px" >  
  </div>
  <div class="col-sm-5">
    <h1>Yang Chen</h1>
    <p>
      <i>Ph.D. Candidate</i><br /> 
      <a href="https://ic.gatech.edu/" target="_blank">Georgia Institute of Technology</a><br />
      [Email]: yangc[at]gatech.edu<br />
      [<a href="https://scholar.google.com/citations?user=o-oBMWEAAAAJ&hl=en" target="_blank">Google Scholar</a>]
      <!-- [<a href="https://twitter.com/ychenNLP" target="_blank">Twitter</a>] -->
      <!-- [<a href="https://drive.google.com/file/d/17wRWA4Nspj_RcVaLLZCae_n7pdHlNoKv/view?usp=sharing" target="_blank">CV</a>] -->
      <br />
      <!-- <p style="color: rgb(51, 38, 150);">Searching for industry lab positions starting in Summer 2024.</p> -->
    </p>
  </div>
</div>


<p> 
<<<<<<< Updated upstream
  I'm a CS Ph.D. candidate at <a href="https://www.ic.gatech.edu/academics/phd-programs" target="_blank">Georgia Tech</a> under the supervision of <a href="http://aritter.github.io/" target="_blank">Prof. Alan Ritter</a> and <a href="https://cocoxu.github.io/" target="_blank"> Prof. Wei Xu</a> working on:
  
  <!-- <br> -->
  <br>
  <!-- <p>Multimodal large language models, retrieval-augmented generation, multilingual, responsible AI:</p> -->
<ul>
    <li>Multimodal: training/evaluating multimodal LLMs + RAG (<a href="https://open-vision-language.github.io/infoseek/" target="_blank">EMNLP'23</a>, <a href="https://open-vision-language.github.io/oven/" target="_blank">ICCV'23</a>, <a href="https://arxiv.org/abs/2311.17136" target="_blank">UniIR</a>)</li>
    <li>Responsible AI: aligning and redteaming multimodal LLMs to protect user <a href="https://arxiv.org/abs/2310.02224" target="_blank">privacy</a> and prevent misinformation (<a href="https://arxiv.org/abs/2212.09683" target="_blank">ACL'23</a>) </li>
    <li>Multilingual: low-resource language synthetic data generation and pre-training (<a href="https://openreview.net/forum?id=DayPQKXaQk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions)" target="_blank">ICLR'24</a>, <a href="https://arxiv.org/abs/2211.15613" target="_blank">ACL'23(Findings)</a>,
      <a href="https://arxiv.org/abs/2004.14519v4" target="_blank">EMNLP'20</a>/<a href="https://arxiv.org/abs/2010.06127" target="_blank">21</a>) </li>
    
</ul>
Previously interned at <a href="https://www.deepmind.com/research" target="_blank">Google Deepmind</a> working on multimodal LLMs/retrieval with <a href="https://www.hexianghu.com/" target="_blank">Hexiang Hu</a> and <a href="https://mingweichang.org/" target="_blank">Ming-Wei Chang</a>. 
</a>
<br>
<br>
Recent Activities:
<ul>
  <li> 2023/12 - Talk at <a href="https://cohere.com/events/c4ai-Yang-Chen-2023" target="_blank">Cohere AI</a> and <a href="https://jina.ai/news/a-tale-of-two-worlds-emnlp-2023-at-sentosa/" target="_blank">Jina AI</a>.
</ul>
=======
  I'm a CS Ph.D. candidate at <a href="https://www.ic.gatech.edu/academics/phd-programs" target="_blank">Georgia Tech</a> under the supervision of <a href="http://aritter.github.io/" target="_blank">Prof. Alan Ritter</a> and <a href="https://cocoxu.github.io/" target="_blank"> Prof. Wei Xu</a>.
  
  <!-- <br> -->
  <!-- <br> -->

<!-- <ul> -->
    <!-- <li>Multimodal: world knowledge w/ multimodal LLMs + RAG (<a href="https://open-vision-language.github.io/infoseek/" target="_blank">EMNLP'23</a>, <a href="https://open-vision-language.github.io/oven/" target="_blank">ICCV'23</a>, <a href="https://arxiv.org/abs/2311.17136" target="_blank">UniIR</a>)</li> -->
    <!-- <li>Responsible AI: aligning and redteaming multimodal LLMs to protect user <a href="https://arxiv.org/abs/2310.02224" target="_blank">privacy</a> </li> -->
    <!-- <li>Multilingual: low-resource language synthetic data generation (<a href="https://arxiv.org/abs/2211.15613" target="_blank">ACL'23(Findings)</a>,  -->
      <!-- <a href="https://arxiv.org/abs/2305.13582" target="_blank">arXiv'23</a>,  -->
      <!-- <a href="https://arxiv.org/abs/2004.14519v4" target="_blank">EMNLP'20</a>/<a href="https://arxiv.org/abs/2010.06127" target="_blank">21</a>) </li> -->
    
<!-- </ul> -->
<!-- Previously interned at <a href="https://www.deepmind.com/research" target="_blank">Google Deepmind</a> working on multimodal LLMs/retrieval with <a href="https://www.hexianghu.com/" target="_blank">Hexiang Hu</a> and <a href="https://mingweichang.org/" target="_blank">Ming-Wei Chang</a>.  -->
<!-- </a>. -->
>>>>>>> Stashed changes

</p>


<!-- <hr>
<h2> Preprint </h2> 

<style>
  .paper::after {
    content: "";
    display: table;
    clear: both;
  }
</style>

<ul>
  <li class="paper">
    <img src="imgs/llm-access.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  
    <div>
      <span class="title">
        <b>Can Language Models be Instructed to Protect Personal Information?</b><br>
      </span>
      Yang Chen<sup>*</sup>, <a href="https://ethanm88.github.io/" target="_blank">Ethan Mendes</a><sup>*</sup>, <a href="https://www.sauvikdas.com/" target="_blank">Sauvik Das</a>, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a>, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a><br>
      <a href="https://arxiv.org/abs/2310.02224" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://llm-access-control.github.io/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a>
  </div>
  </li>
</ul> -->



<hr>
<h2> Publications  </h2> 
<ul>
  <li class="paper">
    <img src="imgs/infoseek.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  
    <div>
      <span class="title">
        <b>Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?</b><br>
      </span>
      <span style="font-size: smaller;">
      Yang Chen, <a href="https://www.hexianghu.com/" target="_blank">Hexiang Hu</a>, <a href="https://luanyi.github.io/" target="_blank">Yi Luan</a>, <a href="https://scholar.google.com/citations?user=opSHsTQAAAAJ&hl=en" target="_blank">Haitian Sun</a>, <a href="https://schangpi.github.io/" target="_blank">Soravit Changpinyo</a>, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a>, <a href="https://mingweichang.org/" target="_blank">Ming-Wei Chang</a><br>
      EMNLP 2023, <a href="https://arxiv.org/abs/2302.11713" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://open-vision-language.github.io/infoseek/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a>, <a href="https://github.com/open-vision-language/infoseek" style="color: #000000" target="_blank">[<u>Dataset</u>]</a>
    </span>
  </div>
  </li>
</ul>
  <button id="toggleButton">Show More Papers</button>
  <div id="hiddenPapers" style="display:none;"><li class="paper">
<li class="paper">
    <img src="imgs/oven.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
    <div>
      <span class="title">
        <b>Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities</b><br>
      </span>
      <span style="font-size: smaller;">
      <a href="https://www.hexianghu.com/" target="_blank">Hexiang Hu</a>,  <a href="https://luanyi.github.io/" target="_blank">Yi Luan</a>, Yang Chen, <a href="https://urvashik.github.io/" target="_blank">Urvashi Khandelwal</a>, <a href="https://mandarjoshi90.github.io/" target="_blank">Mandar Joshi</a>, <a href="https://kentonl.com/" target="_blank">Kenton Lee</a>, <a href="http://kristinatoutanova.com/" target="_blank">Kristina Toutanova</a>, <a href="https://mingweichang.org/" target="_blank">Ming-Wei Chang</a><br>
      ICCV 2023 (Oral), <a href="https://arxiv.org/abs/2302.11154" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://open-vision-language.github.io/oven/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a>, <a href="https://github.com/open-vision-language/oven" style="color: #000000" target="_blank">[<u>Dataset</u>]</a>
    </span>
</li>
<li class="paper">
  <img src="imgs/uniir.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">

  <div>
    <span class="title">
      <b>UniIR: Training and Benchmarking Universal Multimodal Information Retrievers</b><br>
    </span>
    <span style="font-size: smaller;">
    Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a>, Wenhu Chen<br>
    <a href="https://arxiv.org/abs/2311.17136" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://tiger-ai-lab.github.io/UniIR/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a>
    </span>
</div>
</li>

  <li class="paper">
    <img src="imgs/llm-access.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  
    <div>
      <span class="title">
        <b>Can Language Models be Instructed to Protect Personal Information?</b><br>
      </span>
      <span style="font-size: smaller;">
      Yang Chen<sup>*</sup>, <a href="https://ethanm88.github.io/" target="_blank">Ethan Mendes</a><sup>*</sup>, <a href="https://www.sauvikdas.com/" target="_blank">Sauvik Das</a>, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a>, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a><br>
      <a href="https://arxiv.org/abs/2310.02224" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://llm-access-control.github.io/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a>
      </span>
  </div>
  </li>
  <li class="paper">
  <img src="imgs/codec.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  <div>
    <span class="title">
      <b>Constrained Decoding for Cross-lingual Label Projection</b><br>
    </span>
    <span style="font-size: smaller;">
    <a href="https://duonglm38.github.io/" target="_blank">Duong Minh Le</a>, Yang Chen, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a>, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a><br>
    ICLR 2024, <a href="https://openreview.net/forum?id=DayPQKXaQk&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICLR.cc%2F2024%2FConference%2FAuthors%23your-submissions)" style="color: #000000" target="_blank">[<u>OpenReview</u>]</a>
    </span>
</div>
</li>
  <li class="paper">
  <img src="imgs/easyproject.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  <div>
    <span class="title">
      <b>Frustratingly Easy Label Projection for Cross-lingual Transfer</b><br>
    </span>
    <span style="font-size: smaller;">
    Yang Chen, <a href="https://chaojiang06.github.io/" target="_blank">Chao Jiang</a>, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a>, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a><br>
    Findings of ACL 2023, <a href="https://arxiv.org/abs/2211.15613" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://ychennlp-easyproject.hf.space" style="color: #000000" target="_blank">[<u>Demo</u>]</a>, <a href="https://github.com/edchengg/easyproject" style="color: #000000" target="_blank">[<u>Code</u>]</a>
    </span>
</div>
</li>
<li class="paper">
	<img src="imgs/modelselect.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
	<div>
    <span class="title">
      <b>Model Selection for Cross-Lingual Transfer</b><br>
    </span>
    <span style="font-size: smaller;">
    Yang Chen, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a><br>
    EMNLP 2021, <a href="https://arxiv.org/abs/2010.06127" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://github.com/edchengg/model_selection" style="color: #000000" target="_blank">[<u>Code</u>]</a>
    </span>
</div>
 <li class="paper">
  <img alt="Paper 1 Image" height="100" src="imgs/selftrain.png" style="float: left; margin-right: 15px;" width="220"/>
  <div>
  <span class="title">
  <b>Semi-supervised ASR by End-to-end Self-training</b><br/>
  </span>
  <span style="font-size: smaller;">
      Yang Chen, <a href="https://sites.google.com/ttic.edu/weiranwang/home" target="_blank">Weiran Wang</a>, Chao Wang<br/>
      Interspeech 2020, <a href="https://arxiv.org/abs/2001.09128" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>
      </span>
  </div>
  </li>

    <li class="paper">
      <img src="imgs/transfusion.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
    
      <div>
        <span class="title">
          <b>Better Low-Resource Entity Recognition Through Translation and Annotation Fusion</b><br>
        </span>
        <span style="font-size: smaller;">
        Yang Chen, Vedaant Shah, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a><br>
        <a href="https://arxiv.org/abs/2305.13582" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://github.com/edchengg/transfusion" style="color: #000000" target="_blank">[<u>Code</u>]</a>
        </span>
    </div>
    </li>
    <li class="paper">
      <img src="imgs/gigabert.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
      <div>
        <span class="title">
          <b>GigaBERT: Zero-shot Transfer Learning from English to Arabic</b><br>
        </span>
        <span style="font-size: smaller;">
        <a href="https://scholar.google.com/citations?user=rpOgHRMAAAAJ&hl=en" target="_blank">Wuwei Lan</a>, Yang Chen, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a>, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a><br>
        EMNLP 2020, <a href="https://arxiv.org/abs/2004.14519v4" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://github.com/edchengg/GigaBERT" style="color: #000000" target="_blank">[<u>Code</u>]</a>
        </span>
    </div>
    </li>
  <li class="paper">

  <img src="imgs/misinfo.png" alt="Paper 1 Image" width="220" height="100" style="float: left; margin-right: 15px;">
  <div>
    <span class="title">
      <b>Human-in-the-loop Evaluation for Early Misinformation Detection</b><br>
    </span>
    <span style="font-size: smaller;">
    <a href="https://ethanm88.github.io/" target="_blank">Ethan Mendes</a>, Yang Chen, <a href="http://aritter.github.io/index.html" target="_blank">Alan Ritter</a>, <a href="https://cocoxu.github.io/" target="_blank">Wei Xu</a><br>
    ACL 2023, <a href="https://arxiv.org/abs/2212.09683" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>, <a href="https://github.com/ethanm88/hitl-evaluation-early-misinformation-detection" style="color: #000000" target="_blank">[<u>Code</u>]</a>
    </span>
</div>
</li>
  <li class="paper">
  <img alt="Paper 1 Image" height="80" src="imgs/enteval.png" style="float: left; margin-right: 15px;" width="220"/>
  <div>
  <span class="title">
  <b>EntEval: A Holistic Evaluation Benchmark for Entity Representations</b><br/>
  </span>
  <span style="font-size: smaller;">
  <a href="https://ttic.uchicago.edu/~mchen/" target="_blank">Mingda Chen*</a>, <a href="https://zeweichu.github.io/" target="_blank">Zewei Chu*</a>, Yang Chen, <a href="http://karlstratos.com/" target="_blank">Karl Stratos</a>, <a href="https://ttic.uchicago.edu/~kgimpel/index.html" target="_blank">Kevin Gimpel</a><br/>
      EMNLP 2019, <a href="https://arxiv.org/abs/1909.00137" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>
      </span>
  </div>
</li>

  </li><li class="paper">
  <img alt="Paper 1 Image" height="80" src="imgs/postmodifier.png" style="float: left; margin-right: 15px;" width="220"/>
  <div>
  <span class="title">
  <b>PoMo: Generating Entity-Specific Post-Modifiers in Context</b><br/>
  </span>
  <span style="font-size: smaller;">
  <a href="https://www3.cs.stonybrook.edu/~junkang/" target="_blank">Jun Seok Kang</a>, <a href="https://rloganiv.github.io/" target="_blank">Robert L. Logan IV</a>, <a href="https://zeweichu.github.io/" target="_blank">Zewei Chu</a>, Yang Chen, <a href="https://ddua.github.io/src/index.html" target="_blank">Dheeru Dua</a>, <a href="https://ttic.uchicago.edu/~kgimpel/index.html" target="_blank">Kevin Gimpel</a>, <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>, <a href="https://www3.cs.stonybrook.edu/~niranjan/" target="_blank">Niranjan Balasubramanian</a><br/>
      NAACL-HLT 2019, <a href="https://stonybrooknlp.github.io/PoMo/" style="color: #000000" target="_blank">[<u>Project Page</u>]</a> <a href="https://arxiv.org/abs/1904.03111" style="color: #000000" target="_blank">[<u>arXiv</u>]</a>
      </span>
  </div>
  </li>
</ul>
</div>
  <script>
  document.getElementById("toggleButton").addEventListener("click", function() {
      var hiddenDiv = document.getElementById("hiddenPapers");
      if (hiddenDiv.style.display === "none") {
          hiddenDiv.style.display = "block";
          this.innerHTML = "Show Fewer Papers";
      } else {
          hiddenDiv.style.display = "none";
          this.innerHTML = "Show More Papers";
      }
  });
  </script>


<hr>
<h2> Work Experience </h2> 
<ul>
<li class="paper">
	<img src="imgs/gdm-logo.svg" alt="Paper 1 Image" width="220" height="50" style="float: left; margin-right: 15px;">
	<div>
    <span class="title">
      Fall 2022, Spring 2023: Research Intern, <a href="https://www.deepmind.com/" target="_blank">Google Deepmind</a> @ Seattle, WA<br>
    </span>
    Mentor: <a href="https://www.hexianghu.com/" target="_blank">Hexiang Hu</a> and <a href="https://mingweichang.org/" target="_blank">Ming-Wei Chang</a>  <br>
    Multimodal LLMs and Retrieval model
</div>
<li class="paper">
	<img src="imgs/amazon.png" alt="Paper 1 Image" width="220" height="50" style="float: left; margin-right: 15px;">
	<div>
    <span class="title">
      Summer 2019: Applied Scientist Intern, <a href="https://developer.amazon.com/zh/alexa/science" target="_blank">Amazon Alexa</a> @ Boston, MA<br>
    </span>
    Mentor: <a href="https://sites.google.com/ttic.edu/weiranwang/home" target="_blank">Weiran Wang</a> and Chao Wang <br>
    Self-improving speech recognition model <br>
</div>
</ul>

<hr>
<h2> Acknowledgement</h2>
<ul>
  I would like to thank <a href="https://ttic.uchicago.edu/~klivescu/" target="_blank">Professor Karen Livescu</a> and <a href="https://ttic.uchicago.edu/~kgimpel/" target="_blank">Professor Kevin Gimpel</a> at <a href="http://www.ttic.edu" target="_blank">TTIC</a> for giving me the opportunity to start deep learning research in 2018.
</ul>
